{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner](./img/AI_Special_Program_Banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Classification Application - The Naive Bayes Method in `scikit-learn`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will look at a first approach for *text processing* in Python, again using `scikit-learn`. In this context, a thorough pre-processing of the data and *feature extraction* ([feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)) is required. We are also dealing with very sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Insult detection](#Insult-detection)\n",
    "    - [Data preparation](#Data-preparation)\n",
    "    - [Modeling with Naive Bayes classifier](#Modeling-with-Naive-Bayes-classifier)\n",
    "    - [Using a different vectorizer: CountVectorizer](#Using-a-different-vectorizer:-CountVectorizer)\n",
    "- [Learning outcomes](#Learning-outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insult detection\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to recognize insults in discussion forums. The corresponding data set is available in the `data` folder.\n",
    "It was downloaded from the [data repository](https://github.com/ipython-books/cookbook-data) of the [Cookbook](https://ipython-books.github.io/). The required file is `troll.csv`. This was originally provided by the company [Impermium](https://impermium.com) as part of a [Kaggle competition](https://www.kaggle.com/c/detecting-insults-in-social-commentary).\n",
    "\n",
    "***Note:*** Often the specialized package [nltk](http://www.nltk.org/) is used for word processing instead of `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the required packages again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.naive_bayes as nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we read the data into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/troll.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line contains a comment together with its classification as *offensive* (1) or not (0) and the date of the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502172717Z</td>\n",
       "      <td>\"you are both morons and that is never happening\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528164814Z</td>\n",
       "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620142813Z</td>\n",
       "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528205648Z</td>\n",
       "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>20120515200734Z</td>\n",
       "      <td>\"You're all upset, defending this hipster band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult             Date  \\\n",
       "3942       1  20120502172717Z   \n",
       "3943       0  20120528164814Z   \n",
       "3944       0  20120620142813Z   \n",
       "3945       0  20120528205648Z   \n",
       "3946       0  20120515200734Z   \n",
       "\n",
       "                                                Comment  \n",
       "3942  \"you are both morons and that is never happening\"  \n",
       "3943  \"Many toolbars include spell check, like Yahoo...  \n",
       "3944  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...  \n",
       "3945  \"How about Felix? He is sure turning into one ...  \n",
       "3946  \"You're all upset, defending this hipster band...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the *characteristic matrix* $\\mathbf{X}$ and the *classes* (labels) $\\mathbf{y}$. The latter is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Insult']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature matrix, on the other hand, is much more difficult to obtain. `scikit-learn` needs numerical values as inputs, so that the text must be converted into a matrix. This **data preprocessing** (i.e. *data preparation* according to [CRISP-DM](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining)) usually takes place in two steps:\n",
    "1. **Tokenization**: Extracting a **vocabulary**, i.e. a list of words that were used in the text (in our case: in the comments)\n",
    "2. **Counting:** You then count how often the respective word occurs in each data record (also: *document*). As there are generally very many words and only very few of them are actually used in a particular data record (in our case: in a comment), this results in a matrix that mainly contains zeros (i.e. is sparsely populated).\n",
    "\n",
    "The entire process is also referred to as **Bag of Words**. With the help of `scikit-learn` we only need two lines of code for this. The [Tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) method is used for \"counting\", with the help of which words that occur too frequently (\"the\", \"and\", etc.) can be handled adequately. *Tf-idf* normalizes the vocabulary so that words that occur in a large number of data records are given less weight than those that only occur in a small number of data records and are therefore *more specific*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 3947 comments are 16469 different words\n"
     ]
    }
   ],
   "source": [
    "tf = text.TfidfVectorizer()  # tf-idf \n",
    "X_vec = tf.fit(df['Comment'])\n",
    "X = X_vec.transform(df['Comment'])\n",
    "# X = tf.fit_transform(df['Comment'])\n",
    "print(f'In {X.shape[0]} comments are {X.shape[1]} different words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 3947 comments and 16469 different words. Let's take a look at the vocabulary first. Each word found is assigned an *index* in the feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 16397),\n",
       " ('fuck', 5434),\n",
       " ('your', 16405),\n",
       " ('dad', 3409),\n",
       " ('really', 11568),\n",
       " ('don', 4075),\n",
       " ('understand', 14793),\n",
       " ('point', 10754),\n",
       " ('xa0', 15720),\n",
       " ('it', 7048)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not so easy to get a partial dict ... \n",
    "list(X_vec.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the word `dad` has the index 3409 and `really` the index 11568.\n",
    "\n",
    "The feature matrix $X$ is a [scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html). This special data structure stores only those entries that are not $0$ and their coordinates in the matrix with the help of two special index structures (examples can be found in the link above). Let's take a closer look at all this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100269"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.nnz # number of not null entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30831977, 0.20722267, 0.48374632, ..., 0.15469143, 0.07678784,\n",
       "       0.20409929])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.data # not null entries (Tf-Idf weights) as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max() # highest tf-idf-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3409,  5434, 16397, ..., 15294, 16397, 16405])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.indices # indexarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      4,     19, ..., 100202, 100231, 100269])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.indptr # array of index pointer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also interesting to see how sparsely populated the feature matrix is. We can estimate this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature matrix has ~0.15% not null entries.\n"
     ]
    }
   ],
   "source": [
    "print(\"The feature matrix has ~{0:.2f}% not null entries.\".format(\n",
    "          100 * X.nnz / float(X.shape[0] * X.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the classifier again. First we have to split our data into training and test data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=.2, random_state = 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [Multinomial Naive Bayes Classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_na%C3%AFve_Bayes) as a Naive Bayes classifier, which considers the frequency of the words as an integer value. In practice, however, the model also works with the Tf-idf vectorization. In addition, *smoothing* is performed using a parameter $\\alpha$ (an explanation of the approach can be found at [Stanford University](https://nlp.stanford.edu/courses/cs224n/2001/gruffydd/smoothing.html) and we know it as the Laplace estimation ...). Further Naive Bayes models in `scikit-learn` can be found [here](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = ms.GridSearchCV(nb.MultinomialNB(), param_grid={'alpha':np.logspace(-2., 2., 50)})\n",
    "bnb.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does this classifier work on our test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hit rate is around 76.71%\n"
     ]
    }
   ],
   "source": [
    "print(f'The hit rate is around {round(100*bnb.score(X_test, y_test),2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regard to the hit rate, it should be noted that the special division into test and training data, which also includes a random component, plays a role here (a different `random_state` than 17 would also lead to different results). It is also interesting to see which words are most frequently found in offensive comments. This can be found out as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with offensive connotations: \n",
      " ['you' 'your' 'are' 'to' 'the' 'and' 'idiot' 're' 'of' 'fuck' 'that' 'it'\n",
      " 'is' 'like' 'stupid' 'xa0' 'an' 'moron' 'in' 'bitch' 'dumb' 'just' 'go'\n",
      " 'have' 'as' 'not' 'on' 'fucking' 'ass']\n"
     ]
    }
   ],
   "source": [
    "insult_class_prob_sorted = bnb.best_estimator_.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "word_list_insult = np.take(X_vec.get_feature_names_out(), insult_class_prob_sorted[:-30:-1])\n",
    "\n",
    "print(f'Words with offensive connotations: \\n {word_list_insult}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test how well this works with examples we have devised ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(bnb.predict(tf.transform([\n",
    "    \"You are absolutely right.\",\n",
    "    \"This is beyond moronic.\",\n",
    "    \"LOL\"\n",
    "    ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly classifies our self-devised examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a different vectorizer: CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the previously used [Tf-Idf vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), we can also try the simple [Counting vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 3947 comments are 16469 different words\n"
     ]
    }
   ],
   "source": [
    "co = text.CountVectorizer()  # count\n",
    "Xc_vec = co.fit(df['Comment'])\n",
    "Xc = Xc_vec.transform(df['Comment'])\n",
    "# Xc = co.fit_transform(df['Comment'])\n",
    "print(f'In {Xc.shape[0]} comments are {Xc.shape[1]} different words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 16397),\n",
       " ('fuck', 5434),\n",
       " ('your', 16405),\n",
       " ('dad', 3409),\n",
       " ('really', 11568),\n",
       " ('don', 4075),\n",
       " ('understand', 14793),\n",
       " ('point', 10754),\n",
       " ('xa0', 15720),\n",
       " ('it', 7048)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Xc_vec.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100269"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc.nnz # number of not null entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the vocabulary and form of the feature matrix are consistent with what we had before (which was to be expected). The data, on the other hand, are now *frequencies* and no longer Tf-Idf weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc.data # not null entries as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc.max() # highest count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3409,  5434, 16397, ..., 15294, 16397, 16405], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc.indices # index array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      4,     19, ..., 100202, 100231, 100269], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc.indptr # array of index pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature matrix has ~0.15% entries different from 0.\n"
     ]
    }
   ],
   "source": [
    "print(\"The feature matrix has ~{0:.2f}% entries different from 0.\".format(\n",
    "          100 * Xc.nnz / float(Xc.shape[0] * Xc.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now again the division into training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xc_train, Xc_test, yc_train, yc_test) = ms.train_test_split(Xc, y, test_size=.2, random_state = 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the multinomial Naive Bayes method again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnbc = ms.GridSearchCV(nb.MultinomialNB(), param_grid={'alpha':np.logspace(-2., 2., 50)})\n",
    "bnbc.fit(Xc_train, yc_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does this classifier work on our test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hit rate is around 77.85%\n"
     ]
    }
   ],
   "source": [
    "print(f'The hit rate is around {round(100*bnbc.score(Xc_test, yc_test),2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hit rate achieved is higher than before. It can therefore be assumed that words that generally occur frequently in the data have a certain predictive power for the classification problem. Insults often use the personal pronoun \"*you*\", but this is likely to occur frequently in general and is therefore attenuated by the Tf-idf vectorizer. However, this assumption would have to be analysed in more detail by comparing the important words between the two vectorizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again the self-invented examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(bnbc.predict(co.transform([\n",
    "    \"You are absolutely right.\",\n",
    "    \"This is beyond moronic.\",\n",
    "    \"LOL\"\n",
    "    ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models therefore come to the same result in our self-conceived examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning outcomes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important learning objectives of this notebook at a glance:\n",
    "\n",
    "* Extensive data preparation is necessary when building models that work with text data,\n",
    "* Words need to be converted to numeric representations by tokenizing and counting in a bag of words process,\n",
    "* Different vectorization methods count (and therefore weight...) words differently,\n",
    "* The multinomial Naive Bayes classifier can be used to classify word frequencies and its coefficients can be used to check for important words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
