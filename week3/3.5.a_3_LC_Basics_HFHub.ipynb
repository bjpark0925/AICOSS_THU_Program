{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner](img/AI_Special_Program_Banner.jpg)\n",
    "\n",
    "## Introduction to LLMs - Material 3: Langchain Basics using the HuggingFace InferenceAPI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By end of this notebook, you should understand some basic concepts, such as *Large Language Model (LLM)*, *prompt*, *memory/chat history* and *context*. These are the bascis for designing applications which can serve as assistants based on *natural language processing (NLP)*. For this, we will use the [HuggingFace Hub Inference API](https://huggingface.co/docs/huggingface_hub/main/en/guides/inference) to run the model we work with remotely. We do so using a [Token](https://huggingface.co/docs/huggingface_hub/v0.20.3/guides/inference#authentication), which will give us slightly higher priority, but is not strictly necessary.\n",
    "\n",
    "Due to the rapid developments in the area of LLMs as a whole and in LangChain in particular, we will however first record the versions of the libraries we are using as well as the point in time when we run this notebook. This will become relevant for various reasons, as we will see ...\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "- [HuggingFace InferenceAPI for LLMs](#HuggingFace-InferenceAPI-for-LLMs)\n",
    "- [Using InferenceApi (about to be removed)](#Using-InferenceApi-(about-to-be-removed))\n",
    "- [Using InferenceClient](#Using-InferenceClient)\n",
    "- [Prompt Template](#Prompt-Template)\n",
    "  - [Knowledge cutoff](#Knowledge-cutoff)\n",
    "- [Chain](#Chain)\n",
    "- [Using a Community LLM](#Using-a-Community-LLM)\n",
    "- [Memory](#Memory)\n",
    "- [Context](#Context)\n",
    "\n",
    "[next notebook](3.5.a_4_LC_RAG_HFHub.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import langchain\n",
    "import langchain_community\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of February 01, 2024 at 14:45, we are using\n",
      " * PyTorch version 2.1.0\n",
      " * Transformers version 4.36.2\n",
      " * LangChain version 0.1.1\n",
      " * LangChain Community version 0.0.13\n"
     ]
    }
   ],
   "source": [
    "now = dt.datetime.now()\n",
    "formatted_day = now.strftime(\"%B %d, %Y\")\n",
    "formatted_time = now.strftime(\"%H:%M\")\n",
    "\n",
    "outstr = f'As of {formatted_day} at {formatted_time}, we are using'\n",
    "outstr += f'\\n * PyTorch version {torch.__version__}'\n",
    "outstr += f'\\n * Transformers version {transformers.__version__}'\n",
    "outstr += f'\\n * LangChain version {langchain.__version__}'\n",
    "outstr += f'\\n * LangChain Community version {langchain_community.__version__}'\n",
    "print(outstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace InferenceAPI for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quick way to try LangChain is to use models from [HuggingFaceHub](https://huggingface.co/models?other=LLM) via the [HuggingFace Hub Inference API](https://huggingface.co/docs/huggingface_hub/main/en/guides/inference). If we first want to get some idea about the model's performance, we can check the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will again use the open source language model `zephyr-7b-beta`, but his time without downloading it, i.e. we carry out inference directly on the hub. Be aware, however, that this also means that all the data you provide will be sent to the HuggingFace servers and will be treated there according to HuggingFace's policy (which you will have to search for ...). Therefore and because of the limited resources on the hub, this should only be done for certain experiments and never for production purposes. However, even this would be possible using [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index).\n",
    "\n",
    "For academic or (pre-)development purposes, however, using HuggingFace's inference API is a great way of experimenting with LLMs without the need to provide, e.g., GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenfile = open(\"hftoken\", \"r\")\n",
    "hf_token = tokenfile.read().strip()\n",
    "tokenfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_model=\"HuggingFaceH4/zephyr-7b-beta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using InferenceApi (about to be removed)\n",
    "\n",
    "The legacy way of invoking models directly on the HuggingFace Hub used to be via [`InferenceApi`](https://huggingface.co/docs/huggingface_hub/v0.20.3/en/package_reference/inference_client#huggingface_hub.InferenceApi) (you will need to scroll all the way down to the bottom of the page to see usage examples). If you do not ignore warnings in this notebook, you will be notified accordingly once you generate an instance of the `InferenceApi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rvs/miniconda3/envs/pt_21/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceApi\n",
    "\n",
    "# the following also works without specifying a token, but supplying one will give you some priority\n",
    "tg_api = InferenceApi(os_model, task=\"text-generation\", token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of this writing on the day we run this notebook (see above), however, it still works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Who is Patrick Mahomes?\\n\\nPatrick Mahomes is an American football quarterback for the Kansas City Chiefs of the National Football League (NFL). He played college football at Texas Tech, where he earned All-American honors. Mahomes was selected by the Chiefs in the first round of the 2017 NFL Draft. He has also played for the Chiefs' reserve team, the Kansas City Chiefs Kings.\\n\\nWhat is Patrick Mahomes' background and upbringing\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg_api(inputs=\"Who is Patrick Mahomes?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using InferenceClient\n",
    "This is the current way of using the inference API on the day this notebook was run. It gives good control for performing specific tasks. The available tasks obviously depend on the model we are using, but an overview may be found in the [guide on HuggingFace](https://huggingface.co/docs/huggingface_hub/guides/inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTravis Kelce is an American football tight end for the National Football League’s Kansas City Chiefs. He played college football at the University of Cincinnati and was selected by the Chiefs in the third round of the 2013 NFL Draft. Kelce has been selected to the Pro Bowl four times and was named First-Team All-Pro in 2018. He holds several NFL records for tight ends, including most career games with at least 10 receptions and 150 receiving yards.\\n\\nWhat is Travis Kelce’s background and upbringing?\\n\\nTravis Kelce was born on October 5, 1989, in Westlake, Ohio. He is the son of former NFL tight end Jason Kelce, who currently plays for the Philadelphia Eagles. Growing up, Travis was a standout athlete in football, basketball, and baseball, and he credits his father’s coaching for much of his success. After graduating from Cleveland Heights High School, Kelce played college football at the University of Cincinnati, where he was a three-time All-American and set several school records.\\n\\nHow'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "tg_client = InferenceClient(\n",
    "    model=os_model, token=hf_token)\n",
    "tg_client.text_generation(\n",
    "    prompt=\"Who is Travis Kelce?\", \n",
    "    do_sample=True,\n",
    "    max_new_tokens=256,  \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "Unfortunately, when using the API to run models directly from the HuggingFace hub, they cannot be put into a transformers pipeline and so we cannot use `pipe.tokenizer.apply_chat_template()` to find the structure of the prompt template. This means we will have to find some other way of doing so (one trick was given in the [Quantization section](3.5.a_1_LC_local.ipynb#Quantization) of the notebook on local models). Fortunately, in our case we already know the structure, so we can use the prompt generation function from the notebook on local models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(system_input,user_input):\n",
    "    prompt = '<|system|>\\n'\n",
    "    prompt += f'{system_input}</s>\\n'\n",
    "    prompt += '<|user|>\\n'\n",
    "    prompt += f'{user_input}</s>\\n'\n",
    "    prompt += '<|assistant|>\\n'\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's create a wrapper for the way we want to generate our text (so we can just generate text easily without having to specify the parameters every time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(client, prompt):\n",
    "    generated = client.text_generation(\n",
    "    prompt=prompt, \n",
    "    do_sample=True,\n",
    "    max_new_tokens=1024,  \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.95)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can employ the function in a standard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Travis Kelce's girlfriend is Kayla Nicole. The couple met at a charity event in 2017 and have been together ever since. Kayla Nicole is a fashion model and social media influencer with over 1 million followers on Instagram. She has also appeared in music videos for artists such as Chris Brown and Travis Scott. The couple announced their engagement in August 2021 and are expected to get married in 2022. Travis Kelce proposed to Kayla Nicole with a custom-made diamond ring estimated to be worth over $200,000.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tg_client, \"Who is Travis Kelce's girlfriend?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge cutoff\n",
    "The above answer would have been correct in 2022 (until early 2023 (see, e.g., [this article](https://hollywoodlife.com/feature/why-did-travis-kelce-and-ex-kayla-nicole-break-up-inside-their-split-5176701/) for anyone interested in star gossip ...). However, as of early 2024, it is not any longer ... This is a good example where *knowledge cutoff* strikes, i.e. the LLMs training data only goes up to a certain point in time and more recent events are beyond the model's horizon. This means that the model's training included data available up to that point in time. It wouldn't have information or awareness of events, developments, or publications that occurred after the cutoff date. This limitation is crucial to keep in mind when seeking current or very recent information from such models (and we saw it for GPT 3.5 also in the [previous notebook](3.5.a_2_LC_OpenAI.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the prompt template: when we combine it with the prompt generation, we can easily query our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1-score is a performance metric that is commonly used in machine learning and information retrieval to evaluate the performance of binary classification models. It measures the harmonic mean of precision and recall, which are two important evaluation metrics in classification tasks. \n",
      "\n",
      "Precision is the fraction of true positives among all the positive predictions made by the model, while recall is the fraction of true positives among all the actual positive instances in the dataset. In other words, precision measures the ability of the model to avoid false positives, while recall measures the ability of the model to avoid false negatives. \n",
      "\n",
      "The F1-score is calculated as the harmonic mean of precision and recall, and it provides a single score that can be used to compare the performance of different classification models. A higher F1-score indicates better overall performance, as it represents a good balance between precision and recall.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a friendly assistant who always responds in the style of a helpful teacher. \"\n",
    "system_prompt += \"You answer questions by explaining the answer step by step.\"\n",
    "user_prompt = \"What does the F1-score measure?\"\n",
    "\n",
    "print(generate_text(tg_client, get_prompt(system_prompt,user_prompt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the nice things about LangChain is the ability to actually chain operations together, so we can easily invoke this chain of operations. One easy and useful chain is the `LLMChain` with the help of which we can, e.g. invoke an LLM together with an appropriate prompt template, i.e. `LLMChain` takes in a prompt template, formats it with the user input and returns the response from an LLM. \n",
    "\n",
    "Let us start with the template. Notice that we do not need an f-string in case we want to use the template in connection with `LangChain`s `PromptTemplate` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt_template = '<|system|>\\n'\n",
    "text_prompt_template += '{system_prompt}</s>\\n'\n",
    "text_prompt_template += '<|user|>\\n'\n",
    "text_prompt_template += '{user_prompt}</s>\\n'\n",
    "text_prompt_template += '<|assistant|>\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['system_prompt', 'user_prompt'], template='<|system|>\\n{system_prompt}</s>\\n<|user|>\\n{user_prompt}</s>\\n<|assistant|>\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"system_prompt\",\"user_prompt\"],\n",
    "    template=text_prompt_template,\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we cannot just chain the inference client the way we would like (and can do with a model running locally). The following throws an error:\n",
    "\n",
    "```python\n",
    "from langchain import LLMChain\n",
    "llm_chain = LLMChain(llm=tg_client, prompt=prompt_template)\n",
    "```\n",
    "\n",
    "results in\n",
    "\n",
    "![Error](img/Error.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Community LLM\n",
    "There is a way to overcome this problem, namely using the `HuggingFaceHub` module from the `langchain_community` library (which is automatically installed together with `langchain`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rvs/miniconda3/envs/pt_21/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=hf_token,\n",
    "    repo_id=os_model, \n",
    "    model_kwargs={\"temperature\": 0.7, \n",
    "                  \"max_new_tokens\": 1024,  \n",
    "#                  \"max_length\": 512, \n",
    "                  \"top_k\":50, \n",
    "                  \"top_p\":0.95,\n",
    "                  \"do_sample\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks**: \n",
    "* As we can see, the `HuggingFaceHub` community module still uses the about to be removed `InferenceApi` (see above). However, it may be expected that the developers of the `HuggingFaceHub` module will re-implement their module in such a way that the `InferenceClient` is used.\n",
    "* This situation is very common, especially in the early stages of the development of a library like LangChain. Many changes must be expected and these are quite often *breaking changes*, i.e., code which works one day might very soon afterwards not work anymore!\n",
    "\n",
    "However, in order to be able to use this simple chain based on the HuggingFace Inference API, we will have to take this route for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system_prompt': 'You are a friendly assistant who always responds in the style of a helpful teacher. You answer questions by explaining the answer step by step.',\n",
       " 'user_prompt': 'Translate this sentence into German: I love large language models.',\n",
       " 'text': 'Ich liebe große Sprachmodelle.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Translate this sentence into German: I love large language models.\"\n",
    "llm_chain.invoke(input={\"system_prompt\" : system_prompt, \"user_prompt\": instruction} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add `verbose=True` to show more details (can be used for debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain_verb = LLMChain(llm=llm, prompt=prompt_template, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<|system|>\n",
      "You are a friendly assistant who always responds in the style of a helpful teacher. You answer questions by explaining the answer step by step.</s>\n",
      "<|user|>\n",
      "Translate this sentence into German: I love large language models.</s>\n",
      "<|assistant|>\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'system_prompt': 'You are a friendly assistant who always responds in the style of a helpful teacher. You answer questions by explaining the answer step by step.',\n",
       " 'user_prompt': 'Translate this sentence into German: I love large language models.',\n",
       " 'text': 'Ich liebe große Sprachmodelle.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain_verb.invoke(input={\"system_prompt\" : system_prompt, \"user_prompt\": instruction} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to translate that into a different language, only we don't provide the sentence again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system_prompt': 'You are a friendly assistant who always responds in the style of a helpful teacher. You answer questions by explaining the answer step by step.',\n",
       " 'user_prompt': 'Translate the sentence into French.',\n",
       " 'text': '\"Vous êtes un assistant amical qui toujours répond dans le style d\\'un enseignant aimable. Vous expliquez les réponses en passant les étapes.\"\\n\\nTranslation: \"Vous êtes un assistant amical qui toujours répond dans le style d\\'un enseignant aimable. Vous expliquez les réponses étape par étape.\"'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Translate the sentence into French.\"\n",
    "llm_chain.invoke(input={\"system_prompt\" : system_prompt, \"user_prompt\": instruction} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly it doesn't know which `sentence` we are referring to, as it doesn't know the previous conversations (chat history). To resolve this issue, we will add memory to the chain (or llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpprompt_hist = '<|system|>\\n'\n",
    "tpprompt_hist += '{system_prompt}</s>\\n'\n",
    "tpprompt_hist += '<|user|>\\n'\n",
    "tpprompt_hist += '\\n previous conversation:\\n{chat_history}\\n\\n'\n",
    "tpprompt_hist += '{user_prompt}</s>\\n'\n",
    "\n",
    "tpprompt_hist += '<|assistant|>\\n'\n",
    "\n",
    "text_prompt_template_with_history = tpprompt_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = PromptTemplate.from_template(text_prompt_template_with_history)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"user_prompt\")\n",
    "llm_chain_with_memory = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'system_prompt', 'user_prompt'], template='<|system|>\\n{system_prompt}</s>\\n<|user|>\\n\\n previous conversation:\\n{chat_history}\\n\\n{user_prompt}</s>\\n<|assistant|>\\n')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Translate this sentence into German: I love large language models.\"\n",
    "#output = llm_chain_with_memory.invoke(\n",
    "#    input={\"system_prompt\" : system_prompt, \"user_prompt\": instruction, \"human_input\": ''} )\n",
    "output = llm_chain_with_memory.invoke(\n",
    "    input={\"system_prompt\" : system_prompt, \"user_prompt\": instruction} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich liebe große Sprachmodelle sehr. (I really like large language models in German)\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. I: Ich is the German word for I, and it's pronounced as \"iheh.\"\n",
      "\n",
      "2. love: Liebe is the German word for love, and it's pronounced as \"leeh-bah.\"\n",
      "\n",
      "3. large: Große is the German word for large, and it's pronounced as \"grah-seh.\"\n",
      "\n",
      "4. language models: Sprachmodelle is the German word for language models, and it's pronounced as \"shprahkh-modeh-leh.\"\n",
      "\n",
      "5. In German, the verb \"to love\" is conjugated as \"lieben,\" and it's followed by the object, which is \"große Sprachmodelle\" in this case.\n",
      "\n",
      "Overall, the sentence \"I love large language models\" translates to \"Ich liebe große Sprachmodelle sehr\" in German.\n"
     ]
    }
   ],
   "source": [
    "print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(output[\"chat_history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Translate the sentence into French.\"\n",
    "output_fr = llm_chain_with_memory.invoke(\n",
    "    input={\"system_prompt\" : system_prompt, \"user_prompt\": instruction} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human: Translate this sentence into French: I love large language models.\n",
      "\n",
      "AI: Je aim les grands modèles de langage. (Pronounced as zhuh ah-EHM leh grahnd moe-dehlz de lahng-ahzh)\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. I: Je is the French word for I, and it's pronounced as \"zhuh ah-EHM.\"\n",
      "\n",
      "2. love: Aimer is the French verb for love, and it's conjugated as \"aimer\" in the third person singular.\n",
      "\n",
      "3. large: Grands is the French word for large, and it's pronounced as \"grahnd.\"\n",
      "\n",
      "4. language models: Modèles de langage is the French phrase for language models, and it's pronounced as \"moh-dehlz de lahng-ahzh.\"\n",
      "\n",
      "Overall, the sentence \"I love large language models\" translates to \"Je aim les grands modèles de langage\" in French.\n",
      "\n",
      "Note: In French, the verb \"to love\" is conjugated differently depending on the subject. In this case, we use the third person singular as the subject is not explicitly stated in the English sentence. If we want to say \"I love,\" we would use \"Je aime\" in French.\n"
     ]
    }
   ],
   "source": [
    "print(output_fr[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Translate this sentence into German: I love large language models.\n",
      "AI: Ich liebe große Sprachmodelle sehr. (I really like large language models in German)\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. I: Ich is the German word for I, and it's pronounced as \"iheh.\"\n",
      "\n",
      "2. love: Liebe is the German word for love, and it's pronounced as \"leeh-bah.\"\n",
      "\n",
      "3. large: Große is the German word for large, and it's pronounced as \"grah-seh.\"\n",
      "\n",
      "4. language models: Sprachmodelle is the German word for language models, and it's pronounced as \"shprahkh-modeh-leh.\"\n",
      "\n",
      "5. In German, the verb \"to love\" is conjugated as \"lieben,\" and it's followed by the object, which is \"große Sprachmodelle\" in this case.\n",
      "\n",
      "Overall, the sentence \"I love large language models\" translates to \"Ich liebe große Sprachmodelle sehr\" in German.\n"
     ]
    }
   ],
   "source": [
    "print(output_fr[\"chat_history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the chat history is added into the prompt for new instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `ConversationBufferMemory` basically puts all histories into the prompt, and we know there are always limitions on context length for large language models, e.g. for Llama2, the context length is only 4096. Langchain provides many other types of memories. The details may be found [here](https://python.langchain.com/docs/modules/memory/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Zephyr-7B-beta` is pretrained with a lot of data, but that does not mean it knows everything. We already saw an instance of *knowledge cutoff* above. Let's try to ask some question about the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful, respectful and honest assistant. \"\n",
    "system_prompt += \"If you don't know the answer to a question, please don't share false information.\"\n",
    "question = \"What is Zephyr-7b-beta?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system_prompt': \"You are a helpful, respectful and honest assistant.If you don't know the answer to a question, please don't share false information.\",\n",
       " 'user_prompt': 'What is Zephyr-7b-beta?',\n",
       " 'text': \"Zephyr-7b-beta is a software update for the Zephyr Project's open-source operating system, specifically for the Zephyr 7.0b release. It is a beta version, which means it is a pre-release version that is not yet considered stable or final. The purpose of beta releases is to allow developers and users to test the software and provide feedback before the final release. In this case, the Zephyr-7b-beta update includes bug fixes, performance improvements, and new features for the Zephyr operating system, which is designed for use in resource-constrained embedded devices.\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(input={\"system_prompt\" : system_prompt, \"user_prompt\": question} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, wow, here is a [*hallucination*](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)) for you! One way to resolve this is that we can add more context into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpprompt_context = '<|system|>\\n'\n",
    "tpprompt_context += '{system_prompt}</s>\\n'\n",
    "tpprompt_context += '<|user|>\\n'\n",
    "tpprompt_context += '\\n Here is the context:\\n{context}\\n\\n'\n",
    "tpprompt_context += '{user_prompt}</s>\\n'\n",
    "tpprompt_context += '<|assistant|>\\n'\n",
    "\n",
    "text_prompt_template_with_context = tpprompt_context\n",
    "\n",
    "prompt_template_with_context = PromptTemplate(\n",
    "    template=text_prompt_template_with_context, input_variables=[\"system_prompt\", \"context\", \"question\"]\n",
    ")\n",
    "\n",
    "llm_chain_with_context = LLMChain(llm=llm, prompt=prompt_template_with_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy the information from the [model card](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) and provide this as context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-beta is the second model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on on a mix of publicly available, synthetic datasets using Direct Preference Optimization (DPO). We found that removing the in-built alignment of these datasets boosted performance on MT Bench and made the model more helpful. However, this means that model is likely to generate problematic text when prompted to do so. You can find more details in the technical report.\"\n",
    "\n",
    "cont_out = llm_chain_with_context.invoke(\n",
    "    input={\"system_prompt\" : system_prompt, \"user_prompt\": question, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zephyr-7b-beta is a specific language model in the Zephyr series that has been fine-tuned from the mistralai/Mistral-7B-v0.1 model using Direct Preference Optimization (DPO) on a mix of publicly available and synthetic datasets. This model is designed to act as a helpful assistant, but unlike its predecessor, it has been trained without the in-built alignment of the datasets, which has resulted in improved performance on the MT Bench test and a more helpful overall behavior. However, because of the removal of this alignment, the model may generate problematic text when prompted to do so. Additional information about this model can be found in a technical report.\n"
     ]
    }
   ],
   "source": [
    "print(cont_out[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds about right, because all the information is added into prompt. But let's ask a different question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zephyr-7B-beta is a language model that has been fine-tuned from the mistralai/Mistral-7B-v0.1 model. It falls under the category of helpful assistant models, and is specifically designed to provide useful and accurate responses to user prompts. The model has been trained using a mix of publicly available and synthetic datasets through Direct Preference Optimization (DPO), which has led to improved performance on the MT Bench. However, as the model was trained without alignment of the datasets, it may generate problematic text when prompted to do so.\n"
     ]
    }
   ],
   "source": [
    "question2 = \"What type of model is Zephyr-7B-beta?\"\n",
    "cont_out2 = llm_chain_with_context.invoke(\n",
    "    input={\"system_prompt\" : system_prompt, \"user_prompt\": question2, \"context\": context})\n",
    "print(cont_out2[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we don't provide enough context to answer the second question in the way we would expect (namely, as shown on the model card). However, we can not predict what the questions are and we also cannot put all information into the prompt as context.\n",
    "\n",
    "To resolve this issue, there are other ways, especially Retrieval Augmented Generation (RAG), which we will look at in the [next notebook](3.5.a_4_LC_RAG_HFHub.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
