{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner](img/AI_Special_Program_Banner.jpg)\n",
    "\n",
    "## Text Mining - Material 2: Embeddings\n",
    "------\n",
    "This notebook is based on the work of [nlptown](https://github.com/nlptown/nlp-notebooks) and [Matthew Mayo from KDnuggets](https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the breakthroughs of neural networks in Natural Language Processing is the usage of word embeddings. Rather than using the words themselves as features, neural network methods typically take as input dense, relatively low-dimensional vectors that model the meaning and usage of a word. Word embeddings were first popularized through the [Word2Vec](https://arxiv.org/abs/1301.3781) model, developed by Thomas Mikolov and colleagues at Google. Since then, scores of alternative approaches have been developed, such as [GloVe](https://nlp.stanford.edu/projects/glove/) and [FastText](https://fasttext.cc/) embeddings. In this notebook, we'll explore word embeddings with the original Word2Vec approach, as implemented in the [Gensim](https://radimrehurek.com/gensim/) library. \n",
    "\n",
    "You already used an \"Embedding\" layer in PyTorch, which was directly trained within the network. However, we now take a closer look at embeddings and especially focus on the more common libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- [Training word embeddings](#Training-word-embeddings)\n",
    "- [Using word embeddings](#Using-word-embeddings)\n",
    "- [Plotting embeddings](#Plotting-embeddings)\n",
    "- [Clustering embeddings](#Clustering-embeddings)\n",
    "- [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the Corpus. Wikipedia is a good choice for training generic embeddings. Typically you would download the full Wikipedia ([here](https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2)), but since this is over 21GB, just use a [fragment of about 275MB](https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles1.xml-p1p41242.bz2) and store it in your `data` directory. We are using the English Wikipedia here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from gensim.corpora import WikiCorpus # don't forget to install gensim ...\n",
    "\n",
    "wikifile = \"data/Wikipedia/enwiki-latest-pages-articles1.xml-p1p41242.bz2\"\n",
    "corpusfile = \"data/corpus-en.txt\"\n",
    "\n",
    "def make_corpus(in_f, out_f):\n",
    "\n",
    "    \"\"\"Convert Wikipedia xml dump file to text corpus\"\"\"\n",
    "\n",
    "    output = open(out_f, 'w', encoding=\"utf-8\")\n",
    "    wiki = WikiCorpus(in_f)\n",
    "\n",
    "    i = 0\n",
    "    for text in wiki.get_texts():\n",
    "        output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
    "        i = i + 1\n",
    "        if (i % 5000 == 0):\n",
    "            print('Processed ' + str(i) + ' articles')\n",
    "    output.close()\n",
    "    print('Processing of '+ str(i) +' articles complete!')\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    make_corpus(\"data/Wikipedia/enwiki-latest-pages-articles1.xml-p1p41242.bz2\", \"data/corpus-en.txt\")\n",
    "make_corpus(wikifile, corpusfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the result in the `corpus-en.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training word embeddings with Gensim couldn't be easier. The only thing we need is a corpus of sentences in the language under investigation (here German). This means we can feed lists of sentence tokens to Word2Vec by reading the lines in our Wikipedia file and splitting them on spaces (the data has been preprocessed using the WikiCorpus() above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class SentenceCorpus(object):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, \"r\", encoding=\"utf-8\") as i:\n",
    "            for line in i:\n",
    "                tokens = line.strip().split()\n",
    "                yield tokens\n",
    "               \n",
    "sentences = SentenceCorpus(corpusfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our word embeddings, gensim allows us to set a number of parameters. The most important of these are `min_count`, `window`, `size` and `sg`:\n",
    "\n",
    "- `min_count` is the minimum frequency of the words in our corpus. For infrequent words, we just don't have enough information to train reliable word embeddings. It therefore makes sense to set this minimum frequency to at least 10. In these experiments, we'll set it to 100 to limit the size of our model even more.\n",
    "- `window` is number of words to the left and to the right that make up the context that word2vec will take into account.\n",
    "- `vector_size` is the dimensionality of the word vectors. This is generally between 100 and 1000. You often have to make a trade-off: embeddings with a higher dimensionality are able to model more information, but also need more data to train.\n",
    "- `sg`: there are two algorithms to train word2vec: skip-gram and CBOW. Skip-gram tries to predict the context on the basis of the target word; CBOW tries to find the target on the basis of the context. By default, Gensim uses CBOW (`sg=0`).\n",
    "\n",
    "We'll investigate the impact of some of these parameters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences, min_count=100, window=5, vector_size=100, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the model. The word embeddings are on its wordvector (`wv`) attribute, and we can access them by the using the token as key. For example, here is the embedding for German *king*, with the requested 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"king\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily find the similarity between two words. Similarity is measured as the **cosine** between the two word embeddings, and ranges between -1 and +1. The higher the cosine, the more similar two words are. As expected, the figures below show that *king* is closer to *queen* than to *champion*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.similarity(\"king\", \"queen\"))\n",
    "print(model.wv.similarity(\"king\", \"champion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar vein, we can find the words that are most similar to a target word. The words with the most similar embedding to *king* are all similar titles (such as *emperor* or *prince*) or are names of kings (*valdemar*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similar_by_word(\"king\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we can look for words that are similar to a set of words and dissimilar to another set of words at the same time. This allows us to look for analogies of the type *king* is to *man* like ... is to *woman*. \n",
    "This example gives *queen* as the top result - as it should be. However, there are other promising candidates, as you can see. If you want, you can try out a few more examples by yourself -- but please remeber that our training data is very small and results may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['king', 'woman'], negative=[\"man\"], topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can also zoom in on one of the meanings of ambiguous words. For example, the term, *tie* in English can refer to many things: an undecided match, something to waer, or the verb meaning \"to join\" (for further examples, see, e.g., [here](https://www.yourdictionary.com/articles/words-multiple-meanings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"tie\"], topn=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we specify we're looking for words that are similar to *tie* , but dissimilar to *win*, suddenly the best matches are almost all related to dressing up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"tie\"], negative=[\"win\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can present the word2vec model with a list of words and ask it to identify the odd one out. It then uses the word embeddings to identify the word that is least similar to the other ones. For example, in the list *sseminar university king study*, it correctly identifies *king* as the odd one out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.doesnt_match(\"seminar university king study\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize some of our embeddings. To plot embeddings with a dimensionality of 100 or more, we first need to map them to a dimensionality of 2. We do this by using **Principal Component Analysis (PCA)** [(more info)](https://en.wikipedia.org/wiki/Principal_component_analysis). T-SNE, short for t-distributed Stochastic Neighbor Embedding, helps us visualize high-dimensional data by mapping similar data to nearby points and dissimilar data to distance points in the low-dimensional space.\n",
    "\n",
    "T-SNE is present in [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). To run it, we just have to specify the number of dimensions we'd like to map the data to (`n_components`), and the similarity metric that t-SNE should use to compute the similarity between two data points (`metric`). We're going to map to 2 dimensions and use the cosine as our similarity metric. Additionally, we use PCA as an initialization method to remove some noise and speed up computation. The [Scikit-learn user guide](https://scikit-learn.org/stable/modules/manifold.html#t-sne) contains some additional tips for optimizing performance. \n",
    "\n",
    "Plotting all the embeddings in our vector space would result in a very crowded figure where the labels are hardly legible. Therefore we'll focus on a subset of embeddings by selecting the 200 most similar words to a target word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "target_word = \"king\"\n",
    "selected_words = [w[0] for w in model.wv.most_similar(positive=[target_word], topn=200)]\n",
    "embeddings = np.array([model.wv[w] for w in selected_words])\n",
    "\n",
    "mapped_embeddings = TSNE(\n",
    "    n_components=2,\n",
    "    metric='cosine',\n",
    "    init='pca',\n",
    "    learning_rate='auto',\n",
    "    square_distances=True\n",
    ").fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take *king* as our target word, the figure shows some interesting patterns. Notice how \n",
    "* on the top to middle right, the \"words\" represented by roman numerals are clustered together\n",
    "* on the top left, we have a \"north-eastern\" connotation\n",
    "* on the bottom left, we have terms for noblemen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "x = mapped_embeddings[:,0]\n",
    "y = mapped_embeddings[:,1]\n",
    "plt.scatter(x, y)\n",
    "\n",
    "for i, txt in enumerate(selected_words):\n",
    "    plt.annotate(txt, (x[i], y[i]), size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to cluster our embeddings. This can be useful to model semantic information. We'll use agglomerative clustering, a bottom-up clustering method that iteratively takes together the two most similar clusters (or embeddings) in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "vocab = list(model.wv.key_to_index)\n",
    "vectors = [model.wv[w] for w in vocab]\n",
    "vectors_norm = normalize(vectors)\n",
    "\n",
    "clusterer = AgglomerativeClustering(n_clusters=500)\n",
    "clusters = clusterer.fit_predict(vectors_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect some of the clusters. By focusing some of the clusters that contain the names of countries, we can see how these clusters can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dictionary = {}\n",
    "for cluster, word in zip(clusters, vocab): \n",
    "    if cluster not in cluster_dictionary:\n",
    "        cluster_dictionary[cluster] = []\n",
    "    cluster_dictionary[cluster].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cluster_dictionary:\n",
    "    if \"korea\" in cluster_dictionary[x]:\n",
    "        print(cluster_dictionary[x])\n",
    "print(\"\\nAnother cluster:\")\n",
    "for x in cluster_dictionary:\n",
    "    if \"germany\" in cluster_dictionary[x]:\n",
    "        print(cluster_dictionary[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/clusters_nl.tsv\", \"w\", encoding=\"utf-8\") as o:\n",
    "    for c in cluster_dictionary:\n",
    "        for w in cluster_dictionary[c]:\n",
    "            o.write(f\"{w}\\t{c}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are one of the most exciting trends on Natural Language Processing since the 2000s. They allow us to model the meaning and usage of a word, and discover words that behave similarly. This is crucial for the generalization capacity of many machine learning models. Moving from raw strings to embeddings allows them to generalize across words that have a similar meaning, and discover patterns that had previously escaped them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
