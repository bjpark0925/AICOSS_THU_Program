{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAj6ncpIMJaQ"
   },
   "source": [
    "![Banner](img/AI_Special_Program_Banner.jpg)\n",
    "\n",
    "## Recurrent Neural Networks (RNN) - Exercise 1: Tiny Language Model for Limericks - Solution\n",
    "---\n",
    "Instructions are given in <span style=\"color:blue\">blue</span> color.\n",
    "(Unfortunately, Google Colab won't display the blue color.)\n",
    "\n",
    "**Remark**: This solution notebook is for local execution (not on Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tk1sHtZkMJaR"
   },
   "source": [
    "In this exercise, we want to deal with a common requirement when working with deep learning models: Models should be reusable.\n",
    "\n",
    "As you may have noticed, depending on the complexity of the problem and the amount of data, the models' training can take quite some time (when done on a CPU). Therefore, the ability to save and load already trained models is crucial to develop efficient problem solutions.\n",
    "\n",
    "Additionally, we want to make use of the possibility to run notebooks in [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb). The inherent advantage - given over a local execution - is that Colab allows the use of GPUs and, consequently, training time should significantly speed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaClysbTMJaS"
   },
   "source": [
    "**Note**: For this exercise, you will need a Google account. If you don't already have one, please create one using [this website](https://accounts.google.com/signup/v2/webcreateaccount?hl=en&flowName=GlifWebSignIn&flowEntry=SignUp). If you are not comfortable in providing your personal information, don't do so. You can delete your account and associated data after completing the exercise by following [these](https://support.google.com/accounts/answer/32046?hl=en) instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72J8hAr9MJaT"
   },
   "source": [
    "As for the actual contents of this week's exercise, we will look at character-level language modeling with RNNs. In particular, we are trying to train a network capable of producing text that (hopefully) resembles the structure of so-called [Limericks](https://en.wikipedia.org/wiki/Limerick_(poetry))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPw9Jm2ZMJaT"
   },
   "source": [
    "> Deep Thought, after millions of years,\n",
    "<br>\n",
    "> With the ultimate answer appears,\n",
    "<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Which is just 42.\n",
    "<br>\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;This is certainly true,\n",
    "<br>\n",
    "> Though it sounds a bit strange to the ears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePHOZxBOMJaU"
   },
   "source": [
    "<div style=\"text-align: right\"><a href=\"http://www.oedilf.com/db/Lim.php?Word=42\">Source</a></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- [1. Setting up Google Colab](#1.-Setting-up-Google-Colab)\n",
    "- [2. Data Preparation](#2.-Data-Preparation)\n",
    "- [3. RNN Model Creation](#3.-RNN-Model-Creation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bd7S63xyMJaU"
   },
   "source": [
    "## 1. Setting up Google Colab\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qua1qc4wMJaV"
   },
   "source": [
    "* <div style=\"color:blue\">Upload this notebook to Google Colab.</div>\n",
    "\n",
    "    1. Visit https://colab.research.google.com/notebooks/intro.ipynb.\n",
    "    2. Log in to your Google account.\n",
    "    3. Select **`File`** **`Upload notebook`** from the menu bar.\n",
    "    4. Upload this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RQ5o0gVMJaV"
   },
   "source": [
    "* <div style=\"color:blue\">Make sure to utilize GPUs.</div>\n",
    "\n",
    "    1. Select **`Runtime`** **`Change Runtime type`** from the menu bar.\n",
    "    2. Choose *T4 GPU* for hardware acceleration (if that is not already the case; otherwise just cancel the operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure torchtext installed\n",
    "try:\n",
    "    import torchtext\n",
    "except:\n",
    "    print(\"Installing torchtext\")\n",
    "    !pip install torchtext   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B9g91NJWMJaW"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch import nn\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "# force gpu computing, when gpu library is available\n",
    "USE_GPU = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "CENO_ck5MJaZ",
    "outputId": "3200feca-e688-4bcc-c029-bc98b3cdb199"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_AVAILABLE = True\n"
     ]
    }
   ],
   "source": [
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "print(f'CUDA_AVAILABLE = {CUDA_AVAILABLE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G91xAxxsMJae"
   },
   "source": [
    "The data we will work with is provided as simple continuous text. The limericks contained therein have been scraped from [this website](http://www.oedilf.com/db/Lim.php). You will find the text file `limericks.txt` in the `data` folder next to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7faBxMJMJae"
   },
   "source": [
    "* <div style=\"color:blue\">Upload the text file to Google Colab.</div>\n",
    "\n",
    "    1. On the left-hand side of the Google Colab screen, there is a `Files` tab - click to open.\n",
    "    2. You need to be connected to a runtime to see the session storage and run at least one code cell if not already connected.\n",
    "    3. If connected, you can select to upload files to the session storage. Note that the session storage will be purged when the session is terminated.\n",
    "    4. Upload `limericks.txt` to the session storage (this might take a while; to check whether the file has been uploaded, close and re-open the files tab).\n",
    "    5. Run the following code cells to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BZUKSUT2MJae"
   },
   "outputs": [],
   "source": [
    "# Reading data\n",
    "limericks_file = \"data/limericks.txt\"\n",
    "limericks = open(limericks_file, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "AtyTXbcXMJag",
    "outputId": "e294f3b4-147c-4b55-8c37-e9a3758e65c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from the mornings 'til mid-afternoons\n",
      "in luzon you can feel the monsoons.\n",
      "if that weren't enough,\n",
      "here comes nastier stuff:\n",
      "baguios, which are monstrous typhoons.\n",
      "\n",
      "the old hay farmer's crib had backed up,\n",
      "and resembled an overstuffed cup.\n",
      "when he eyeballed the scene,\n",
      "he said, \"what does this mean?\n",
      "there's a balefulness present, a-yup!\"\n",
      "\n",
      "with affamishment, food's what you need.\n",
      "you would fight for a sesame seed.\n",
      "a small crumb or an ort,\n",
      "you'd take any old sort,\n",
      "and you'd polish it off with great speed.\n",
      "\n",
      "for bahaism, oneness is prime:\n",
      "that of god, and of prophets in time,\n",
      "of all peoples and races;\n",
      "its primary place is\n",
      "to end all our warfare and crime.\n",
      "\n",
      "toss out all of those pepcid ac.\n",
      "i am now achlorhydric. ah, me!\n",
      "no more zantac or tums\n",
      "in immeasurable sums ?\n",
      "i'm a hulk of antacid debris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking at some limericks\n",
    "print(limericks[9990:10790])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaN1O2kiMJak"
   },
   "source": [
    "## 2. Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "NEbS7zOEMJak",
    "outputId": "d580235c-56c6-45b9-bf86-c1cf6e95bf46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the unique characters used in the text\n",
    "vocab = sorted(set(limericks))\n",
    "print(vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GOnyRfGLMJam",
    "outputId": "3b6c35c7-03e8-4737-cfc2-43c16ddca92f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 0,\n",
       " '\\n': 1,\n",
       " ' ': 2,\n",
       " '!': 3,\n",
       " '\"': 4,\n",
       " '#': 5,\n",
       " '$': 6,\n",
       " '%': 7,\n",
       " '&': 8,\n",
       " \"'\": 9,\n",
       " '(': 10,\n",
       " ')': 11,\n",
       " '*': 12,\n",
       " '+': 13,\n",
       " ',': 14,\n",
       " '-': 15,\n",
       " '.': 16,\n",
       " '/': 17,\n",
       " '0': 18,\n",
       " '1': 19,\n",
       " '2': 20,\n",
       " '3': 21,\n",
       " '4': 22,\n",
       " '5': 23,\n",
       " '6': 24,\n",
       " '7': 25,\n",
       " '8': 26,\n",
       " '9': 27,\n",
       " ':': 28,\n",
       " ';': 29,\n",
       " '<': 30,\n",
       " '=': 31,\n",
       " '>': 32,\n",
       " '?': 33,\n",
       " '@': 34,\n",
       " '[': 35,\n",
       " '\\\\': 36,\n",
       " ']': 37,\n",
       " '^': 38,\n",
       " '_': 39,\n",
       " 'a': 40,\n",
       " 'b': 41,\n",
       " 'c': 42,\n",
       " 'd': 43,\n",
       " 'e': 44,\n",
       " 'f': 45,\n",
       " 'g': 46,\n",
       " 'h': 47,\n",
       " 'i': 48,\n",
       " 'j': 49,\n",
       " 'k': 50,\n",
       " 'l': 51,\n",
       " 'm': 52,\n",
       " 'n': 53,\n",
       " 'o': 54,\n",
       " 'p': 55,\n",
       " 'q': 56,\n",
       " 'r': 57,\n",
       " 's': 58,\n",
       " 't': 59,\n",
       " 'u': 60,\n",
       " 'v': 61,\n",
       " 'w': 62,\n",
       " 'x': 63,\n",
       " 'y': 64,\n",
       " 'z': 65,\n",
       " '{': 66,\n",
       " '|': 67,\n",
       " '}': 68,\n",
       " '~': 69}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating unique index for each character in the text\n",
    "char_to_index = {char:index for index, char in enumerate(vocab)}\n",
    "char_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6gSydJeiMJan",
    "outputId": "e7929cef-313a-42f8-fc90-229e3a8fbdab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking index of a specific character\n",
    "char_to_index['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "cpLVrAxnMJap",
    "outputId": "f65e9006-012b-40cb-eb67-99f8aff3fb7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*',\n",
       "       '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7',\n",
       "       '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^',\n",
       "       '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
       "       'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
       "       'z', '{', '|', '}', '~'], dtype='<U1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping the index back to the characters\n",
    "index_to_char = np.array(vocab)\n",
    "index_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NgrRANknMJar",
    "outputId": "5c30b64f-012c-4069-dd11-b400f01b803b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking character for a given index\n",
    "index_to_char[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "eE0GVyD4MJas",
    "outputId": "ee271cb7-8d42-488f-91bd-e7785d08b8fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 40, 55, ...,  3,  4,  1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding the entire text\n",
    "encoded_text = np.array([char_to_index[char] for char in limericks])\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zJg87GNMJau"
   },
   "source": [
    "* <div style=\"color:blue\">How many characters are there in the entire text?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "i-7PCisKMJav",
    "outputId": "8482612f-96ac-4764-efb4-a23835a64f2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14933947"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution goes here:\n",
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NijmEuwXMJaw",
    "outputId": "6169c0b2-0d87-478c-da56-73f94a57cbee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cap'n jack was washed over the side.\\nhis crew searched but found not hair nor hide.\\nno longer the he\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing raw text ...\n",
    "limericks[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "TPLNllHXMJay",
    "outputId": "2a08449f-3637-4879-d131-c5e50038502a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 40, 55,  9, 53,  2, 49, 40, 42, 50,  2, 62, 40, 58,  2, 62, 40,\n",
       "       58, 47, 44, 43,  2, 54, 61, 44, 57,  2, 59, 47, 44,  2, 58, 48, 43,\n",
       "       44, 16,  1, 47, 48, 58,  2, 42, 57, 44, 62,  2, 58, 44, 40, 57, 42,\n",
       "       47, 44, 43,  2, 41, 60, 59,  2, 45, 54, 60, 53, 43,  2, 53, 54, 59,\n",
       "        2, 47, 40, 48, 57,  2, 53, 54, 57,  2, 47, 48, 43, 44, 16,  1, 53,\n",
       "       54,  2, 51, 54, 53, 46, 44, 57,  2, 59, 47, 44,  2, 47, 44])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... with encoded text\n",
    "encoded_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC5zyTpjMJa0"
   },
   "source": [
    "The next thing we have to take care of is to split the data into batches. For this, it is important to determine a sequence length for each batch (number of characters) that \n",
    "\n",
    "* is long enough to capture the general structure of a limerick, but\n",
    "* is not too long, so that past information does not simply become noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugquS8jyMJa0"
   },
   "source": [
    "* <div style=\"color:blue\">Take a look at the limericks in <code>limericks.txt</code> and decide on a suitable <b>sequence length</b>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Vgg9mwtKMJa5"
   },
   "outputs": [],
   "source": [
    "sequence_length = 60 # <- your solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42 40 55  9 53  2 49 40 42 50  2 62 40 58  2 62 40 58 47 44 43  2 54 61\n",
      " 44 57  2 59 47 44  2 58 48 43 44 16  1 47 48 58  2 42 57 44 62  2 58 44\n",
      " 40 57 42 47 44 43  2 41 60 59  2 45]  ->  54\n",
      "\"cap'n jack was washed over the side.\\nhis crew searched but f\"  ->  'o'\n"
     ]
    }
   ],
   "source": [
    "class CharSequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text_encoded, seq_length, offset):\n",
    "        sequences = []\n",
    "        target = []\n",
    "        for i in range(0, len(text_encoded) - seq_length - 1, offset):\n",
    "            seq = text_encoded[i: i + seq_length]\n",
    "            sequences.append(seq)\n",
    "            target.append(text_encoded[i + seq_length])\n",
    "        self.sequences = torch.tensor(sequences)\n",
    "        self.target = torch.tensor(target)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.target[idx]\n",
    "    \n",
    "\n",
    "offset = sequence_length \n",
    "\n",
    "ds = CharSequenceDataset(encoded_text, sequence_length, offset)\n",
    "\n",
    "## inspection:\n",
    "for seq, target in ds:\n",
    "    print(seq.numpy(), ' -> ', target.numpy())\n",
    "    print(repr(''.join(index_to_char[seq])), \n",
    "          ' -> ', repr(''.join(index_to_char[target])))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DTr-0om2MJbF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248899"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final preparation step: create a data loader\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "trainloader = DataLoader(ds, shuffle=True, batch_size=BATCH_SIZE)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8jcSqcIMJbG"
   },
   "source": [
    "## 3. RNN Model Creation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gTeMUfqnMJbH"
   },
   "outputs": [],
   "source": [
    "# Vocabulary length\n",
    "vocab_size = len(vocab)\n",
    "# Embedding dimensions\n",
    "embed_dim = 64\n",
    "# Number of RNN units (neurons)\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lFMJR-OUMJbI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (embedding): Embedding(70, 64)\n",
       "  (rnn): LSTM(64, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=70, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, recurrent_type='LSTM'):\n",
    "        super(Network, self).__init__()\n",
    "        self.recurrent_type = recurrent_type\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if recurrent_type == 'RNN':\n",
    "            self.rnn = nn.RNN(embedding_dim, rnn_units, batch_first=True)\n",
    "        elif recurrent_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, rnn_units, batch_first=True)\n",
    "        elif recurrent_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, rnn_units, batch_first=True)\n",
    "        self.linear = nn.Linear(rnn_units, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        #lstm also returns the cell hidden state\n",
    "        if self.recurrent_type == \"LSTM\":\n",
    "            output, (hidden, cell_hidden) = self.rnn(embedded)\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded)\n",
    "        output = self.linear(hidden[-1])\n",
    "        return output\n",
    "    \n",
    "# Instantiate the PyTorch model (and remember the type for saving the model ...)\n",
    "rnn_type = 'LSTM'\n",
    "model = Network(vocab_size, embed_dim, rnn_units, recurrent_type=rnn_type)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pmi5OdnmMJbM"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train() \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Iterate over the training dataset\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "def train_for_epochs(epchs, model, trainloader, optimizer, criterion):\n",
    "    history = {\n",
    "        \"loss\": []\n",
    "    }\n",
    "    loop = trange(epchs)\n",
    "    # Training loop\n",
    "    for epoch in loop:\n",
    "\n",
    "        epoch_loss_train = train(model, trainloader, optimizer, criterion)   \n",
    "\n",
    "        history[\"loss\"].append(epoch_loss_train)\n",
    "\n",
    "        loop.set_description(f\"Train Loss: {epoch_loss_train:.4f}\")\n",
    "    return history\n",
    "\n",
    "# record the total number of epochs for training so far (may be used for naming models ...)\n",
    "tot_epochs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec5EpLDzMJbO"
   },
   "source": [
    "* <div style=\"color:blue\">Train the model for <b>2</b> epochs. Double-check that you enabled a GPU runtime (unless you don't mind waiting). However, even if you are using GPUs, it may take a while to complete the training.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "t0X1nhNqMJbO",
    "outputId": "87b4325a-2132-4d08-c4d7-86ea2d5ab0fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for training the network\n",
      "Charlevel training started: 2024-01-28 23:36:10.053410\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c68d88141e48028d259b8d0cc955db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charlevel training finished 2024-01-28 23:37:15.730771 (duration: 0:01:05.677361)\n"
     ]
    }
   ],
   "source": [
    "# Your solution goes here:\n",
    "NUM_EPOCHS = 2 \n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "print(f'Using {device} for training the network')\n",
    "\n",
    "# Move the model weight to the desired device\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# run training\n",
    "start_train = dt.datetime.now()\n",
    "print(f'Charlevel training started: {start_train}')\n",
    "history = train_for_epochs(NUM_EPOCHS, model, trainloader, optimizer, criterion)\n",
    "finish_train = dt.datetime.now()\n",
    "print(f'Charlevel training finished {finish_train} (duration: {finish_train - start_train})')\n",
    "\n",
    "tot_epochs += NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVq5ItpdMJbP"
   },
   "source": [
    "* <div style=\"color:blue\">Save the state dict of your model to your session storage.</div>\n",
    "\n",
    "**Hint**: Have a look at the [documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html) to save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "E5y5jf8hMJbP"
   },
   "outputs": [],
   "source": [
    "# Your solution goes here:\n",
    "torch.save(model.state_dict(), f\"models/model_{rnn_type}_{tot_epochs:02}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46aSFOJeMJbR"
   },
   "source": [
    "* <div style=\"color:blue\">Continue training for another <b>18</b> epochs. In total you should have trained the model for <b>20</b> epochs.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "Nkg9Ugw7MJbR",
    "outputId": "c24619c1-57f3-42bf-818c-e03e6d82c0c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue charlevel training started: 2024-01-28 23:37:15.754693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889386ad68ff43e2b9da7c88e43d5d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue charlevel training finished 2024-01-28 23:47:26.008792 (duration: 0:10:10.254099)\n"
     ]
    }
   ],
   "source": [
    "# Your solution goes here:\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# run training\n",
    "start_train = dt.datetime.now()\n",
    "print(f'Continue charlevel training started: {start_train}')\n",
    "history = train_for_epochs(NUM_EPOCHS, model, trainloader, optimizer, criterion)\n",
    "finish_train = dt.datetime.now()\n",
    "print(f'Continue charlevel training finished {finish_train} (duration: {finish_train - start_train})')\n",
    "\n",
    "tot_epochs += NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--LExjVsMJbS"
   },
   "source": [
    "* <div style=\"color:blue\">Save the state of of your model to your session storage, but use a different file name this time.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "arWyWow0MJbT"
   },
   "outputs": [],
   "source": [
    "# Your solution goes here:\n",
    "torch.save(model.state_dict(), f\"models/model_{rnn_type}_{tot_epochs:02}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUfsjS8bMJbU"
   },
   "source": [
    "* <div style=\"color:blue\">Download both saved models from your session storage to your local hard drive (place them in the <code>models</code> folder).</div>\n",
    "\n",
    "**CAUTION**: If you accidentally close the current session without downloading the models first, those will be purged and your training progress is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <div style=\"color:blue\">Continue with <a href=\"3.3.b_RNN_Ex_2_Sol.ipynb\">exercise 2</a>. (Either locally or in Colab is fine.)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EX_05a_RNNs_Sol.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
