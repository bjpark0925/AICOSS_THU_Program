{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner](img/AI_Special_Program_Banner.jpg)\n",
    "\n",
    "## Introduction to LLMs - Material 6: Prompt engineering revisited\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is an idea: what if it is good enough that the LLM can generate text and we can achieve our goal by prompt engineering alone? So, we will give our local approach another try.\n",
    "\n",
    "---\n",
    "<div style=\"color:red\"><b>Attention:</b></div> \n",
    "Do <b>not</b> try to run this notebook on the university server! Or, if you try to do it, make sure no one else is using the same GPU as you are!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- [Prompt engineering revisited](#Prompt-engineering-revisited)\n",
    "  - [Text classification by prompt engineering](#Text-classification-by-prompt-engineering)\n",
    "  - [Initial tries](#Initial-tries)\n",
    "  - [Looking at the movie reviews](#Looking-at-the-movie-reviews)\n",
    "- [Learning Outcomes](#Learning-Outcomes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification by prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by checking our available GPU resources again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  0%   47C    P8              17W / 450W |     28MiB / 24564MiB |      0%      Default |\n",
      "|    0   N/A  N/A      1228      G   /usr/lib/xorg/Xorg                           18MiB |\n"
     ]
    }
   ],
   "source": [
    "# this will only work on Linux ...\n",
    "!nvidia-smi | grep MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is enough space, we can proceed and import the necessary libraries ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and instantiate the model in the form of a [transformers pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e83c93f661745bca9ee5a63270a0e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_pipe = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=\"HuggingFaceH4/zephyr-7b-beta\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device=0,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=100,  \n",
    "    temperature=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_llm = HuggingFacePipeline(pipeline=cls_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(system_input,user_input):\n",
    "    prompt = '<|system|>\\n'\n",
    "    prompt += f'{system_input}</s>\\n'\n",
    "    prompt += '<|user|>\\n'\n",
    "    prompt += f'{user_input}</s>\\n'\n",
    "    prompt += '<|assistant|>\\n'\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the prompt that worked for the model accessed via the InferenceAPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_sys_prompt = \"Answer with only one word, either 'positive' or 'negative', \"\n",
    "cls_sys_prompt += \"depending on the sentiment of the opinion.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial tries\n",
    "\n",
    "The positive one first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive.\n"
     ]
    }
   ],
   "source": [
    "opinion = \"Brock Purdy is an awesome QB!\"\n",
    "print(cls_llm.invoke(get_prompt(cls_sys_prompt, opinion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this looks promising. Now for a negative one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "opinion = \"I think the Niners QB stinks!\"\n",
    "print(cls_llm.invoke(get_prompt(cls_sys_prompt, opinion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now on to the movies ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/movie_data.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70's, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"Murder in Greenwich\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\n"
     ]
    }
   ],
   "source": [
    "print(df.review.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: none\n",
      "\n",
      "Negative: murder, covered-up, disgrace, lack of emotion\n",
      "\n",
      "Based on the analysis of the sentiment of the opinion, the answer would be 'negative'.\n"
     ]
    }
   ],
   "source": [
    "opinion = df.review.iloc[0]\n",
    "print(cls_llm.invoke(get_prompt(cls_sys_prompt, opinion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, not only do we have a misclassification (which, by the reasoning provided by the LLM seems understandable, however), but the output becomes more verbose again. Therefore, while it does not seem completely useless, we should probably do some more prompt engineering or play around with the hyper parameters (maybe even temperature?). This is left for the interested student to try out, but the presentation is now finished.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "The contents presented in the series of notebooks on using LangChain to work with LLMs touched on some of the most relevant aspects of this task. Having worked through the notebooks, you should now\n",
    "* realize that LLMs are made for generating text, but may also have other areas of application like, e.g., text classification\n",
    "* have an idea about the consequences of *knowledge cutoff*\n",
    "* know that there are proprietary as well as open source LLMs available, which may be used via an API or (in the case of open source LLMs) locally\n",
    "* be aware of the opportunity to experiment with models via HuggingFace's InferenceAPI\n",
    "* have a firm grip on the concepts of *prompt engineering*, *memory*, and *context*\n",
    "* know how to employ *vector databases* for *Retrieval Augmented Generation* (RAG)\n",
    "* be aware of the option of *fine-tuning* an LLM to a specific task\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
