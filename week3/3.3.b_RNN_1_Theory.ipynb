{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner](img/AI_Special_Program_Banner.jpg)\n",
    "\n",
    "# Recurrent Neural Networks (RNN) - Material 1: Some Background\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The material presented is adapted from *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019 (code available on [GitHub](https://github.com/rasbt/python-machine-learning-book-3rd-edition))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks (RNN)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the CNNs presented previously, there are a number of other popular neural network architectures to solve a variety of different real-world applications.\n",
    "\n",
    "Many of these applications deal with so-called **sequential data** and, for example, a wanted prediction for the next element(s) of a given sequence. A group of machine learning algorithms that is particularly suitable to tackle such challenges are **Recurrent Neural Networks**.\n",
    "\n",
    "Arguably, the most popular research and application domains for RNNs are found in **Natural Language Processing (NLP)** and **Time Series Analysis**. While you will dive even deeper into NLP as we go along, *Time Series Analysis* is beyond the scope of this course.\n",
    "\n",
    "To start our NLP adventure, our focus here will be to get you covered with some basic ideas regarding RNNs and to develop a proper understanding of how those networks work.\n",
    "\n",
    "As usual, we will also do some hands-on implementations using `PyTorch`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- [Modeling sequential data](#Modeling-sequential-data)\n",
    "  - [Representing sequences](#Representing-sequences)\n",
    "  - [Sequence modeling variations](#Sequence-modeling-variations)\n",
    "    - [Examples](#Examples)\n",
    "- [RNNs for modeling sequences](#RNNs-for-modeling-sequences)\n",
    "  - [RNN looping mechanism](#RNN-looping-mechanism)\n",
    "  - [Computing activations in an RNN](#Computing-activations-in-an-RNN)\n",
    "  - [Alternate recurrence models](#Alternate-recurrence-models)\n",
    "  - [Long short-term memory cells (LSTM)](#Long-short-term-memory-cells-(LSTM))\n",
    " \n",
    "$\\rightarrow$ &nbsp; [Project 1: Sentiment Analysis](3.3.b_RNN_2_Sentiment.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling sequential data\n",
    "---\n",
    "* Special properties of sequential data\n",
    "  + Previous assumption:  independent and identically distributed data\n",
    "    * Spam detection (text): what does this assumption mean, and is it realistic?\n",
    "    * Time series: e.g., stock market values $\\rightarrow$ assumption is not valid\n",
    "  + Sequential data: **order** matters!\n",
    "* Some sort of **memory** is necessary to work with sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing sequences\n",
    "\n",
    "Consider *sequence* $[\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},\\dots,\\mathbf{x}^{(T)}]$ of length $T$ where superscript represents *order*, e.g.\n",
    "* time-series: $\\mathbf{x}^{(t)}$ represents inputs at time $t$\n",
    "* text: $\\mathbf{x}^{(t)}$ represents\n",
    "  - $t$-th word\n",
    "  - $t$-th character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In figure:\n",
    "* $\\mathbf{x}^{(t)}$: input features, $y^{(t)}$: target labels\n",
    "\n",
    "<img src=\"./img/16_01.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequence modeling variations\n",
    "* many $\\equiv$ sequence, one $\\equiv$ fixed-size vector or scalar (aka rank-0 / rank-1 [*tensor*](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html); following this link is highly recommended!)\n",
    "<img src=\"./img/16_02.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples\n",
    "\n",
    "* Many-to-one: e.g., *sentiment analysis* using text data as input and a class label as output\n",
    "* One-to-many: e.g., *image captioning* - input is an image and output is an image description (text)\n",
    "* Many-to-many: \n",
    "  * synchronized: e.g., video classification, where each frame in a video is labeled\n",
    "  * delayed: e.g., translating entire sentences from one language to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNNs for modeling sequences\n",
    "---\n",
    "* typical RNN structure includes a *recursive* component\n",
    "* hidden layer receives its input from both the input layer of the current time step **and** the hidden layer from the previous time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RNN looping mechanism\n",
    "* Using past outputs allows for memory\n",
    "* outputs may be the *tensor* $\\mathbf{o}^{(T)}$ or the *sequence*  $[\\mathbf{o}^{(0)},\\mathbf{o}^{(1)},\\dots,\\mathbf{o}^{(T)}]$\n",
    "\n",
    "<img src=\"./img/16_03.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* multiple hidden layers possible\n",
    "* representation can be *unfolded* for better understanding\n",
    "\n",
    "<img src=\"./img/16_04.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ hidden units now receive *two* inputs:\n",
    "* net preactivation from input layer (as in standard ANNs)\n",
    "* activation of hidden layer from *previous time step* (additionally)\n",
    "* in multi-layer case, hidden layer $\\mathbf{h}_l^{(t)}$ receives input from\n",
    "  + output $\\mathbf{o}_{l-1}^{(t)}$ of *previous layer* at *current time step*\n",
    "  + $\\mathbf{h}_l^{(t-1)}$ hidden values from *previous time step*\n",
    "* practical consequence: *inner* recurrent layers *must return a sequence as output*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Computing activations in an RNN\n",
    "\n",
    "* two-part weight matrix\n",
    "  * $\\mathbf{W}_{hh}$: The weight matrix associated with the recurrent edge (from old hidden layer to new one)\n",
    "  * $\\mathbf{W}_{xh}$: the weight matrix between the (new) input and the hidden layer\n",
    "  * $\\mathbf{W}_{ho}$: the weight matrix between the hidden layer and the output layer\n",
    "* sometimes $\\mathbf{W}_{xh}$ and $\\mathbf{W}_{hh}$ concatenated to $\\mathbf{W}_h$:\n",
    "  $\\mathbf{W}_h = [\\mathbf{W}_{xh},\\mathbf{W}_{hh}]$\n",
    "* weight updates and activation happen behind the scenes when using ```PyTorch```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./img/16_05.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate recurrence models\n",
    "\n",
    "* so far, only *hidden recurrence*, i.e., hidden layer has recurrent property\n",
    "* also possible: *output recurrence*, i.e., recurrent connection from output layer $\\mathbf{o}^{(t-1)}$\n",
    "  + to current hidden layer $\\mathbf{h}^{(t)}$ or\n",
    "  + to current output layer $\\mathbf{o}^{(t)}$\n",
    "* depending on recurrence model, need weight matrix\n",
    "  * $\\mathbf{W}_{hh}$: for hidden recurrence\n",
    "  * $\\mathbf{W}_{oh}$: for output recurrence (output-to-hidden)\n",
    "  * $\\mathbf{W}_{oo}$: for output recurrence (output-to-output)\n",
    "* sometimes written as $\\mathbf{W}_{\\text{rec}}$ in catch-all fashion\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/16_07.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long short-term memory cells (LSTM)\n",
    "\n",
    "* Introduced by [Schmidhuber et al.](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory)\n",
    "* avoids vanishing / exploding gradients (which present a *major problem* when learning long-range interactions; see [Pascanu et. al.: *vanishing* or *exploding gradients*](https://arxiv.org/pdf/1211.5063.pdf))\n",
    "* Representation of / replacement for hidden layer\n",
    "* Cell has \n",
    "  + *state* $\\mathbf{C}^{(t)}$ (for which a *candidate value* $\\tilde{\\mathbf{C}}^{(t)}$ is first computed), \n",
    "  + gates (*forget gate* $\\mathbf{f}^{(t)}$, *input gate* $\\mathbf{i}^{(t)}$, and *output gate* $\\mathbf{o}^{(t)}$) as well as\n",
    "  + *hidden units* $\\mathbf{h}^{(t)}$\n",
    "* Implementation is (once again) provided by```PyTorch```\n",
    "\n",
    "<img src=\"./img/16_09.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ let's try this in practice on [Sentiment analysis](3.3.b_RNN_2_Sentiment.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
