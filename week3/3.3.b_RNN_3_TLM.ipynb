{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner](img/AI_Special_Program_Banner.jpg)\n",
    "\n",
    "## Recurrent Neural Networks (RNN) - Material 3: Tiny Language Model (Character Level Training)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The material presented is adapted from *Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019 (code available on [GitHub](https://github.com/rasbt/python-machine-learning-book-3rd-edition))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here: TLM (*Tiny* Language Model) instead of LLMs (Large Language Models) popularized by ChatGPT\n",
    "* Many principles are the same, especially *randomness* (possibly leading to *hallucinations* with LLMs)\n",
    "* Goal: generate text by \"teaching\" the computer a language by feeding it texts to learn from\n",
    "* Approach: do this character-by-character (see [Sutskever et al.](https://icml.cc/2011/papers/524_icmlpaper.pdf))!\n",
    "* Great read: [Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- [Visualizing the idea](#Idea-visualized)\n",
    "- [Preprocessing](#Preprocessing-the-dataset)\n",
    "- [Classification Problem](#Problem-formulation-as-classification-task)\n",
    "- [Building a character-level RNN model](#Building-a-character-level-RNN-model)\n",
    "- [Evaluation phase: generating new text passages](#Preparing-the-evaluation-phase:-generating-new-text-passages)\n",
    "- [Text generation](#Text-Generation)\n",
    "- [Model Training](#Iterative-model-training)\n",
    "- [Learning Outcomes](#Learning-Outcomes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure torchtext installed\n",
    "try:\n",
    "    import torchtext\n",
    "except:\n",
    "    print(\"Installing torchtext\")\n",
    "    !pip install torchtext   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch import nn\n",
    "from tqdm.notebook import trange, tqdm\n",
    "# force gpu computing, when gpu library is available\n",
    "USE_GPU = True\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see below, there is no ground truth here in the same sense as it occurs in other examples. Therefore, we need a special form of the training loop, since it does not make sense to calculate *validation accuracy* here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing module for training loop\n",
    "from train_loop import train_for_epochs_no_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings (use 'default' instead of 'ignore' if you want to see the warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_AVAILABLE = True\n"
     ]
    }
   ],
   "source": [
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "print(f'CUDA_AVAILABLE = {CUDA_AVAILABLE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea visualized\n",
    "* break text down into sequence of characters\n",
    "* then three step approach:\n",
    "  - data preparation\n",
    "  - build RNN model\n",
    "  - sampling for generating new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/16_11.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset\n",
    "* step 0: data (works by Mark Twain) obtained from [Project Gutenberg](https://www.gutenberg.org/)\n",
    "* step 1: find list of *unique* characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 5197033\n",
      "Total Length: 5196942\n",
      "Unique Characters: 106\n"
     ]
    }
   ],
   "source": [
    "## Reading and processing text\n",
    "with open('data/Twain.txt', 'r', encoding='utf8') as fp:\n",
    "    text=fp.read()\n",
    "    \n",
    "start_idx = text.find('A CONNECTICUT YANKEE')\n",
    "end_idx = text.find('END OF THIS PROJECT GUTENBERG EBOOK')\n",
    "print(start_idx, end_idx)\n",
    "\n",
    "text = text[start_idx:end_idx]\n",
    "char_set = set(text)\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* step 2: create mappings to encode and decode the text\n",
    "\n",
    "<img src=\"./img/16_12.png\" width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (5196942,)\n",
      "A CONNECTICUT Y      == Encoding ==>  [28  1 30 42 41 41 32 30 47 36 30 48 47  1 52]\n",
      "[28 41 38 32 32  1]  == Reverse  ==>  ANKEE \n"
     ]
    }
   ],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "\n",
    "text_encoded = np.array(\n",
    "    [char2int[ch] for ch in text],\n",
    "    dtype=np.int32)\n",
    "\n",
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(char_array[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 -> A\n",
      "1 ->  \n",
      "30 -> C\n",
      "42 -> O\n",
      "41 -> N\n"
     ]
    }
   ],
   "source": [
    "#transform to tensor\n",
    "ds_text_encoded = torch.from_numpy(text_encoded)\n",
    "\n",
    "for i in range(5):\n",
    "    ex = ds_text_encoded[i]\n",
    "    print(f'{ex.numpy()} -> {char_array[ex.numpy()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem formulation as classification task\n",
    "---\n",
    "* Given a sequence of text, we want to predict the next character\n",
    "* $\\rightarrow$ multiclass with number of classes $=$ number of unique characters\n",
    "<img src=\"./img/16_13.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $\\rightarrow$ iterative generation of text\n",
    "+ inputs and targets offset by one\n",
    "\n",
    "<img src=\"./img/16_14.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in PyTorch\n",
    "---\n",
    "* decide on *length of sequence* to use for training\n",
    "  + longer sequences $\\rightarrow$ more meaningful sentences, but possibly harder to capture long-term dependencies\n",
    "  + shorter sequences $\\rightarrow$ correct on individual words, but ignoring context\n",
    "* $\\rightarrow$ hyperparameter to be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28  1 30 42 41 41 32 30 47 36 30 48 47  1 52 28 41 38 32 32  1 36 41  1\n",
      " 38 36 41 34  1 28 45 47 35 48 45  5 46  1 30 42]  ->  48\n",
      "\"A CONNECTICUT YANKEE IN KING ARTHUR'S CO\"  ->  'U'\n"
     ]
    }
   ],
   "source": [
    "class CharSequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text_encoded, seq_length, offset):\n",
    "        sequences = []\n",
    "        target = []\n",
    "        for i in range(0, len(text_encoded) - seq_length - 1, offset):\n",
    "            seq = text_encoded[i: i + seq_length]\n",
    "            sequences.append(seq)\n",
    "            target.append(text_encoded[i + seq_length])\n",
    "        self.sequences = torch.tensor(sequences)\n",
    "        self.target = torch.tensor(target)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.target[idx]\n",
    "    \n",
    "\n",
    "seq_length = 40\n",
    "offset = seq_length \n",
    "\n",
    "ds = CharSequenceDataset(text_encoded, seq_length, offset)\n",
    "\n",
    "## inspection:\n",
    "for seq, target in ds:\n",
    "    print(seq.numpy(), ' -> ', target.numpy())\n",
    "    print(repr(''.join(char_array[seq])), \n",
    "          ' -> ', repr(''.join(char_array[target])))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129923"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final preparation step: create a data loader\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "trainloader = DataLoader(ds, shuffle=True, batch_size=BATCH_SIZE)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for training the network\n"
     ]
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "print(f'Using {device} for training the network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a character-level RNN model\n",
    "---\n",
    "* no activation in the final FC layer $\\rightarrow$ logits available for sampling later\n",
    "* we will try out the GRU architecture this time\n",
    "* bi-directionality is obviously not needed for this specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (embedding): Embedding(106, 26)\n",
       "  (rnn): GRU(26, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=106, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, recurrent_type='LSTM'):\n",
    "        super(Network, self).__init__()\n",
    "        self.recurrent_type = recurrent_type\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if recurrent_type == 'RNN':\n",
    "            self.rnn = nn.RNN(embedding_dim, rnn_units, batch_first=True)\n",
    "        elif recurrent_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, rnn_units, batch_first=True)\n",
    "        elif recurrent_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, rnn_units, batch_first=True)\n",
    "        self.linear = nn.Linear(rnn_units, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        #lstm also returns the cell hidden state\n",
    "        if self.recurrent_type == \"LSTM\":\n",
    "            output, (hidden, cell_hidden) = self.rnn(embedded)\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded)\n",
    "        # take the last time step and predict the next character\n",
    "        output = self.linear(hidden[-1])\n",
    "        return output\n",
    "\n",
    "vocab_size = len(char_array)\n",
    "# Define hyperparameters\n",
    "embedding_dim = vocab_size // 4\n",
    "rnn_units = 512\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model = Network(vocab_size, embedding_dim, rnn_units, recurrent_type='GRU')\n",
    "# Move the model weights to the desired device\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for accumulating the loss values over the epochs\n",
    "hist_cum = []\n",
    "# counting the numbe rof epochs trained so far\n",
    "total_epochs = 0\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the evaluation phase: generating new text passages\n",
    "* can convert logits to probabilities via [softmax function](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "* conversion just *scaling by a constant* $\\rightarrow$ logits can be used directly for choosing most probable character\n",
    "* for generating new text: need *(random) sampling*: [torch.multinomial](https://pytorch.org/docs/stable/generated/torch.multinomial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: tensor([[0.3333, 0.3333, 0.3333]])\n",
      "tensor([[1, 2, 1, 1, 1, 0, 1, 0, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# example 1: equal probabilities\n",
    "logits = torch.tensor([[1.0, 1.0, 1.0]])\n",
    "print('Probabilities:', torch.softmax(logits, 1))\n",
    "\n",
    "# Define the number of samples to draw\n",
    "num_samples = 10\n",
    "\n",
    "# Use torch.multinomial to sample from the distribution\n",
    "samples = torch.multinomial(torch.exp(logits), num_samples, replacement=True)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: tensor([[0.1065, 0.1065, 0.7870]])\n",
      "tensor([[1, 2, 2, 2, 2, 2, 0, 2, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "# example 2: higher probability for sample 2\n",
    "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
    "print('Probabilities:', torch.softmax(logits, 1))\n",
    "\n",
    "# Define the number of samples to draw\n",
    "num_samples = 10\n",
    "\n",
    "# Use torch.multinomial to sample from the distribution\n",
    "samples = torch.multinomial(torch.exp(logits), num_samples, replacement=True)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sample(model, starting_str, \n",
    "           len_generated_text=500, \n",
    "           max_input_length=40,\n",
    "           scale_factor=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "    encoded_input = [char2int[s] for s in starting_str]\n",
    "    encoded_input = torch.tensor(encoded_input).view(1, -1).to(device)\n",
    "\n",
    "    generated_str = starting_str\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len_generated_text):\n",
    "            logits = model(encoded_input)\n",
    "            logits = logits.squeeze(0)\n",
    "\n",
    "            scaled_logits = logits * scale_factor\n",
    "            new_char_idx = torch.multinomial(torch.exp(scaled_logits), num_samples=1)\n",
    "\n",
    "            new_char_idx = new_char_idx[-1].item()\n",
    "\n",
    "            generated_str += str(char_array[new_char_idx])\n",
    "\n",
    "            new_char_idx = torch.tensor([new_char_idx]).view(1, 1).to(device)\n",
    "            encoded_input = torch.cat(\n",
    "                [encoded_input, new_char_idx],\n",
    "                dim=1)\n",
    "            encoded_input = encoded_input[:, -max_input_length:]\n",
    "\n",
    "    return generated_str\n",
    "# print(sample(model, starting_str=\"Huck didn't think\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature: Predictability vs. randomness\n",
    "* scaling of logits before computing softmax possible\n",
    "* scaling factor $\\alpha$ can be interpreted as *inverse temperature* in physics\n",
    "  + $\\alpha > 1$: getting cooler $\\rightarrow$ more predictable (distribution becomes more distinct)\n",
    "  + $\\alpha > 1$: getting warmer $\\rightarrow$ more random (distribution becomes more uniform)\n",
    "* tradeoff between sensibility and uniqueness of generated sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities before scaling:         tensor([[0.1065, 0.1065, 0.7870]])\n",
      "Probabilities after scaling with 1.5: tensor([[0.0453, 0.0453, 0.9094]])\n",
      "Probabilities after scaling with 0.1: tensor([[0.3104, 0.3104, 0.3792]])\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
    "\n",
    "print('Probabilities before scaling:        ', torch.softmax(logits, 1))\n",
    "print('Probabilities after scaling with 1.5:', torch.softmax(1.5 * logits, 1))\n",
    "print('Probabilities after scaling with 0.1:', torch.softmax(0.1 * logits, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating samples of different characteristics**\n",
    "\n",
    "Choose the (inverse) temperature value (`scale_factor`) as follows:\n",
    "* $\\approx 2.0$ for predictable results\n",
    "* $\\approx 0.5$ for highly random results\n",
    "* $\\approx 1.5$ for reasonable results\n",
    "\n",
    "Obviously, this only holds once the model has reached a certain maturity, i.e., it has been trained for a reasonable amount of epochs. However, we can use the `sample` function to get a better idea as to how the model learns. We can even check what happens, if we apply the the model right after initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "This can be performed after various stages of training (just re-run the cell below after performing training). It is particularly interesting to do so right after initialization of the model and compare that to the output after just one epoch. What do you notice? Also, try continuing the training for some more epochs and see what happens then. Finally, play around with the (inverse) temperature a bit (values to try out could be $2.0, 1.5$ and $0.5$, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing sampling after 1 epochs of training with (inverse) temperature set to 2.0\n",
      "\n",
      "Low temperature:\n",
      "========================\n",
      "Huck didn't think the manted their not we the had cansers, and here were not got of heid with the rearsing\n",
      "the stood the stand the proses proce menty the was good of the with dever lafsed the masterss, and here were was the some of\n",
      "coneresteres and mand the crooded of the was be pernessing the shother the hald not the king\n",
      "court king the was the prowed then as they was no the there and lith\n",
      "the precter he the geat in the keve worked hive never of the wanthers the seet of the is got like a forsterts and here from\n"
     ]
    }
   ],
   "source": [
    "inverse_temperature = 2.\n",
    "temp_cat = \"Low temperature\"\n",
    "if inverse_temperature <= 1.5001:\n",
    "    if inverse_temperature <= 0.8001:\n",
    "        temp_cat = \"High temperature\"\n",
    "    else:\n",
    "        temp_cat = \"Normal temperature\"\n",
    "\n",
    "print(f'Performing sampling after {total_epochs} epochs of training with (inverse) temperature set to {inverse_temperature:.1f}')\n",
    "print(f\"\\n{temp_cat}:\\n========================\")\n",
    "print(sample(model, starting_str=\"Huck didn't think\", \n",
    "             scale_factor=inverse_temperature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the training loop once and go back to check what output the model produces then. Afterwards, continue training and go back and forth between sampling and training to get a feeling for the progress the model makes towards a TML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charlevel training started: 2024-01-28 23:24:44.114913\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2839ff52b3457f985535d3083a1bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charlevel training finished 2024-01-28 23:24:53.853363 (duration: 0:00:09.738450)\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1 \n",
    "# run training\n",
    "start_train = dt.datetime.now()\n",
    "print(f'Charlevel training started: {start_train}')\n",
    "history = train_for_epochs_no_gt(device, NUM_EPOCHS, model, trainloader, optimizer, criterion)\n",
    "finish_train = dt.datetime.now()\n",
    "print(f'Charlevel training finished {finish_train} (duration: {finish_train - start_train})')\n",
    "#accumulate loss values\n",
    "hist_cum += history['loss']\n",
    "total_epochs += NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the loss curve\n",
    "Finally, we can plot the loss curve to get a feeling as to whether the model is still improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been trained for 1 epochs so far\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'The model has been trained for {total_epochs} epochs so far\\n')\n",
    "# plot the loss curve\n",
    "x_arr = np.arange(len(hist_cum)) + 1\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "plt.plot(x_arr, hist_cum, '-o', label='Train loss')\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "---\n",
    "With the conclusion of the chapter on RNNs, you should now\n",
    "\n",
    "* know the differences between sequential and non-sequential data;\n",
    "* have an idea in which areas of application RNNs are a suitable machine learning technique;\n",
    "* have a solid understanding of the key concepts that differentiate RNNs from other network architectures;\n",
    "* have the ability to describe an LSTM cell and what issues it overcomes;\n",
    "* be comfortable with a variety of NLP-related data preprocessing techniques, such as tokenization, encoding, and embeddings;\n",
    "* be able to perform sentiment analysis utilizing RNNs;\n",
    "* train a model to generate text in the style of works presented a learning material;\n",
    "* have the ability to implement RNNs on your own, with the help of `PyTorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
