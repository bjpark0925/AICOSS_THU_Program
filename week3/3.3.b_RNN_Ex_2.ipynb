{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner](img/AI_Special_Program_Banner.jpg)\n",
    "\n",
    "## Recurrent Neural Networks (RNN) - Exercise 2: Creating Limericks\n",
    "---\n",
    "Instructions are given in <span style=\"color:blue\">blue</span> color.\n",
    "(Unfortunately, Google Colab won't display the blue color.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This exercise is a continuation of [exercise 1](3.3.b_RNN_Ex_1.ipynb). Please make sure that you have completed it in advance. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- [Idea](#Idea)\n",
    "- [Task 1: Load models trained beforehand](#Task-1:-Load-models-trained-beforehand)\n",
    "- [Task 2: Use and evaluate models](#Task-2:-Use-and-evaluate-models)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "We want to use the tiny language models we created and saved earlier to actually create limeriks (or at least try them out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch import nn\n",
    "from tqdm.notebook import trange, tqdm\n",
    "# force gpu computing, when gpu library is available\n",
    "USE_GPU = True\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_AVAILABLE = True\n",
      "Doing inference with the trained networks using cuda\n"
     ]
    }
   ],
   "source": [
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "print(f'CUDA_AVAILABLE = {CUDA_AVAILABLE}')\n",
    "\n",
    "# Set the device for inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "print(f'Doing inference with the trained networks using {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "limericks_file = \"data/limericks.txt\"\n",
    "limericks = open(limericks_file, 'r').read()\n",
    "vocab = sorted(set(limericks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating unique index for each character in the text\n",
    "char_to_index = {char:index for index, char in enumerate(vocab)}\n",
    "# Mapping the index back to the characters\n",
    "index_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary length\n",
    "vocab_size = len(vocab)\n",
    "# Embedding dimensions\n",
    "embed_dim = 64\n",
    "# Number of RNN units (neurons)\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Load models trained beforehand\n",
    "* <div style=\"color:blue\">Define the same Network Class as in exercise 1.</div>\n",
    "* <div style=\"color:blue\">Load both your trained models into separate model objects and move them to the desired device.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here:\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, recurrent_type='LSTM'):\n",
    "        ...\n",
    "    def forward(self, x):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (embedding): Embedding(70, 64)\n",
       "  (rnn): LSTM(64, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=70, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Use and evaluate models\n",
    "* <div style=\"color:blue\">Use your models and the provided helper function <code>generate_limericks</code> to create some text. Evaluate the quality of both models. Were they able to replicate the structure of a limerick? What about rhyme schemes, grammar, and content?</br>\n",
    "<b>Hint:</b> You might want to play around with the <code>length</code> parameter when generating the limericks</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate text based on a given start sequence\n",
    "def generate_limericks(model, start_sequence, sequence_length, length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "    encoded_input = [char_to_index[s] for s in start_sequence]\n",
    "    encoded_input = torch.tensor(encoded_input).view(1, -1).to(device)\n",
    "\n",
    "    generated_str = start_sequence\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "            logits = model(encoded_input)\n",
    "            logits = logits.squeeze(0)\n",
    "\n",
    "            scaled_logits = logits * temperature\n",
    "            new_char_indx = torch.multinomial(torch.exp(scaled_logits), num_samples=1)\n",
    "\n",
    "            new_char_indx = new_char_indx[-1].item()\n",
    "\n",
    "            generated_str += str(index_to_char[new_char_indx])\n",
    "\n",
    "            new_char_indx = torch.tensor([new_char_indx]).view(1, 1).to(device)\n",
    "            encoded_input = torch.cat(\n",
    "                [encoded_input, new_char_indx],\n",
    "                dim=1)\n",
    "            encoded_input = encoded_input[:, -sequence_length:]\n",
    "\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your solution (answer) goes here*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
