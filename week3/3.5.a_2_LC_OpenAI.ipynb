{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner](img/AI_Special_Program_Banner.jpg)\n",
    "\n",
    "## Introduction to LLMs - Material 2: Using the OpenAI API with Langchain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- [A proprietary LLM: OpenAI](#A-proprietary-LLM:-OpenAI)\n",
    "  - [Building an OpenAI chain](#Building-an-OpenAI-chain)\n",
    " \n",
    "[next notebook](3.5.a_3_LC_Basics_HFHub.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A proprietary LLM: OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main reasons of using LangChain is that it provides a standard interface for interacting with many different LLMs.  Apart from OpenAI, langchain also has integrations to many other language models, in particular open source ones from HuggingFace, as we saw in the [previous notebook](3.5.a_1_LC_local.ipynb) (see the [full list]( https://python.langchain.com/docs/integrations/llms/)). Here, we will access the OpenAI API using a free account (which has limited quota in the equivalent of 5 USD per month). To do so, we need an [API key](https://platform.openai.com/api-keys) and we can then use OpenAI models with LangChain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "#from langchain.llms import HuggingFacePipeline\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard template for OpenAI is well known, so we just use it here together with LangChain's `PromptTemplate` class and an `LLMChain` (which we will discuss in more detail in the [next notebook](3.5.a_3_LC_Basics_HFHub.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "        template=template,\n",
    "    input_variables=['question']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an OpenAI chain\n",
    "This process is quite straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenfile = open(\"oai-coss-token\", \"r\")\n",
    "oai_token = tokenfile.read().strip()\n",
    "tokenfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "davinci = OpenAI(openai_api_key=oai_token, temperature=1.0, max_tokens=512)\n",
    "gpt35 = OpenAI(openai_api_key=oai_token, model_name=\"gpt-3.5-turbo-instruct\", temperature=1.0, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "#    llm=davinci\n",
    "    llm=gpt35\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Who is Travis Kelce?', 'text': \"Travis Kelce is a professional American football player, currently playing tight end for the Kansas City Chiefs of the National Football League (NFL). He was drafted by the Chiefs in the third round of the 2013 NFL Draft and has played his entire career with the team. Kelce is a two-time First-Team All-Pro selection and six-time Pro Bowler. He has also won multiple awards, including the NFL's 2019 Offensive Player of the Year and the 2020 Super Bowl Champion.\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is Travis Kelce?\"\n",
    "print(llm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"Who is Travis Kelce's girlfriend?\", 'text': \"Travis Kelce's girlfriend is Kayla Nicole.\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is Travis Kelce's girlfriend?\"\n",
    "print(llm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the LLM does not seem to be quite up to date ... another issue to discuss in the [next notebook](3.5.a_3_LC_Basics_HFHub.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
